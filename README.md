# Prompt Tuning for CLIP-Based Action Recognition

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

Research on prompt tuning techniques for video action recognition using CLIP-based vision-language models.

## ðŸ“Š Results

**[View Complete Results â†’](RESULTS.md)**

Comprehensive results document with all experimental tables, figures, and key findings including:
- UCF101, Kinetics400, and HMDB51 performance metrics
- Robustness analysis and augmentation effects
- Computational cost analysis
- Model calibration and explainability visualizations

All experimental data is available as Python dictionaries in [src/tables/results_data.py](src/tables/results_data.py) for easy programmatic access and custom analyses. See [docs/RESULTS_DATA.md](docs/RESULTS_DATA.md) for the complete API reference and [example_usage.py](example_usage.py) for usage examples.

To regenerate the results document:
```bash
python generate_results_readme.py
```

## Project Organization

```
â”œâ”€â”€ LICENSE            <- Open-source license
â”œâ”€â”€ Makefile           <- Makefile with convenience commands
â”œâ”€â”€ README.md          <- The top-level README for developers
â”œâ”€â”€ RESULTS.md         <- Comprehensive results document (auto-generated)
â”œâ”€â”€ generate_results_readme.py  <- Script to regenerate RESULTS.md
â”œâ”€â”€ example_usage.py   <- Examples of using results data programmatically
â”œâ”€â”€ requirements.txt   <- Requirements file for reproducing the environment
â”‚
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed
â”‚   â”œâ”€â”€ processed      <- Final, canonical data sets for modeling
â”‚   â”‚   â”œâ”€â”€ prompts    <- Generated prompts
â”‚   â”‚   â””â”€â”€ results    <- Model predictions and evaluations
â”‚   â””â”€â”€ raw            <- Original, immutable data dump
â”‚
â”œâ”€â”€ docs               <- Documentation (mkdocs format)
â”‚   â”œâ”€â”€ mkdocs.yml     <- MkDocs configuration
â”‚   â”œâ”€â”€ RESULTS_DATA.md <- API reference for results data
â”‚   â””â”€â”€ docs/          <- Documentation pages
â”‚
â”œâ”€â”€ figures            <- Generated visualizations
â”‚   â”œâ”€â”€ calibration    <- Model calibration plots
â”‚   â”œâ”€â”€ explainer      <- Attention rollout visualizations
â”‚   â””â”€â”€ gflops         <- Computational cost analysis
â”‚
â”œâ”€â”€ hpc_submit         <- HPC job submission scripts
â”‚   â”œâ”€â”€ data           <- Data preparation jobs
â”‚   â”œâ”€â”€ evaluation     <- Model evaluation jobs
â”‚   â”œâ”€â”€ explainability <- Explainability analysis jobs
â”‚   â”œâ”€â”€ train_models   <- Model training jobs
â”‚   â””â”€â”€ utilities      <- Utility scripts
â”‚
â”œâ”€â”€ models             <- Trained and serialized models
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks and analysis scripts
â”‚   â”œâ”€â”€ data           <- Data exploration notebooks
â”‚   â”œâ”€â”€ evaluation     <- Model evaluation notebooks
â”‚   â”œâ”€â”€ explainability <- Explainability analysis notebooks
â”‚   â”œâ”€â”€ tables         <- Table generation scripts for results
â”‚   â””â”€â”€ train_models   <- Training notebooks
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and explanatory materials
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX
â”‚   â””â”€â”€ figures        <- Generated graphics for reporting
â”‚
â”œâ”€â”€ src                <- Source code for this project
â”‚   â”œâ”€â”€ __init__.py    <- Makes src a Python module
â”‚   â”œâ”€â”€ plots.py       <- Visualization utilities
â”‚   â”œâ”€â”€ configs        <- Configuration files
â”‚   â”œâ”€â”€ data           <- Data loading and processing scripts
â”‚   â”œâ”€â”€ eval           <- Evaluation utilities
â”‚   â”œâ”€â”€ modeling       <- Model architectures and training code
â”‚   â””â”€â”€ tables         <- Table generation utilities
â”‚       â””â”€â”€ results_data.py  <- Central data store for all experimental results
â”‚
â”œâ”€â”€ tests              <- Unit tests
â”‚
â””â”€â”€ tokens             <- API tokens and credentials
```

--------

