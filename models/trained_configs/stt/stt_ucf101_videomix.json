{
  "clip-model": "openai/clip-vit-base-patch16",
  "ctx-len": 8,
  "ctx-len-video": 8,
  "class-wise": false,
  "lr": 8e-05,
  "weight-decay": 0.2,
  "std-init": 0.02,
  "num-temporal-views": 4,
  "num-spatial-views": 3,
  "num-frames-temporal": 8,
  "regularisation-strength": 0.0,
  "regularisation-text": "A video of a person doing",
  "cosine-regularisation-strength": 0.0,
  "use-handcrafted-features": false,
  "keep-vision-prompts-throughout": false,
  "temporal-pooling": "mean",
  "num-heads-attention-pooling": 1,
  "num-frames": 8,
  "has-discriminative-conditioning": false,
  "space-between-frames": null,
  "epochs": 20,
  "start-train-step": 0,
  "epochs-to-skip": 0,
  "save-every-n-steps": 10000000000.0,
  "continue-from": null,
  "grad-norm-max": 1.0,
  "loss-function": "cross-entropy",
  "dataset-config": {
    "batch-size": 4,
    "num-workers": 2,
    "batches-per-backprop": 32,
    "split-file": "",
    "K-train": null,
    "K-val": null,
    "K-test": null,
    "split-function": "",
    "random-seed": 42,
    "has-validation": true,
    "use-cache": false,
    "resized-cache-protocol": null,
    "save-to-cache": false,
    "class-protocol": "all",
    "num-classes": null,
    "use-excess-for": "none",
    "remove-class-if-insufficient": false,
    "class-weights": null,
    "skip-first-K-train": 0,
    "skip-first-K-val": 0,
    "val-proportion": 0.2,
    "use-clips": false
  },
  "use-checkpointing": false,
  "debug": false,
  "use-profiling": false,
  "use-fresh-lr-scheduler": true,
  "learn-just-pooling": false,
  "checkpoint-last-n": 12,
  "log-every-n-steps": 10,
  "early-stop-patience": 3,
  "training-history": {
    "train-acc": [
      0.09465072372561359,
      0.28986784140969163,
      0.39672750157331654,
      0.47400881057268723,
      0.518313404657017,
      0.5574575204531151,
      0.6066708621774701,
      0.6315921963499056,
      0.6489616110761485,
      0.6703587161736941,
      0.6989301447451227,
      0.6891126494650723,
      0.7204531151667716,
      0.7317809943360604,
      0.7439899307740717,
      0.7395846444304595,
      0.756702328508496,
      0.747136563876652,
      0.7684078036500944,
      0.7638766519823789
    ],
    "train-loss": [
      4.39048045154185,
      3.4026394843455003,
      2.967993578901825,
      2.6790574358873505,
      2.49772090347703,
      2.3654332520453116,
      2.2341783354310887,
      2.1136831832127125,
      2.0274090180538074,
      1.9606338990717431,
      1.8658344477658906,
      1.8222455897970422,
      1.7576693036107616,
      1.7237755172278162,
      1.6783244916220894,
      1.6458925375629327,
      1.6189796058842039,
      1.6105681393565134,
      1.5838079914647578,
      1.576208995437382
    ],
    "val-acc": [
      0.23992443324937027,
      0.4357682619647355,
      0.5503778337531486,
      0.6253148614609572,
      0.7191435768261965,
      0.7229219143576826,
      0.7959697732997482,
      0.8066750629722922,
      0.8180100755667506,
      0.853904282115869,
      0.8457178841309824,
      0.8759445843828715,
      0.8790931989924433,
      0.8942065491183879,
      0.8954659949622166,
      0.9055415617128464,
      0.9105793450881612,
      0.9105793450881612,
      0.9149874055415617,
      0.9124685138539043
    ],
    "val-loss": [
      3.1760395346045196,
      2.139361553550968,
      1.6918704604509496,
      1.3464248396775285,
      1.1058943712595917,
      1.0076415147327393,
      0.7755696297193917,
      0.7147121253524267,
      0.6697006880526222,
      0.5678052944131196,
      0.5874865410910446,
      0.5128051784485977,
      0.49207806437433477,
      0.4515715492725072,
      0.42690725543862323,
      0.3976938448452229,
      0.38528480472615556,
      0.3911277301414806,
      0.37955635094015816,
      0.3842278552788218
    ]
  },
  "fp16": false,
  "grad-scaler-config": {
    "init-scale": 8192,
    "growth-factor": 2.0,
    "backoff-factor": 0.5,
    "growth-interval": 100
  },
  "activation-clamp-value": 10000.0,
  "videomix-type": "spatial",
  "videomix-prob": 1,
  "use-augmentation": false,
  "method-name": "stt",
  "num-frame-tokens": 1,
  "num-heads-summary-attention": 12,
  "layers-with-summary-to-text": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9
  ],
  "prompt-optimisation-method": "stt",
  "dataset": "ucf101",
  "data-dir": "/work3/fasco/data/raw/datasets/pevogam/ucf101/versions/1/UCF101/UCF-101",
  "model": "openai/clip-vit-base-patch16",
  "K-train": null,
  "K-val": null,
  "K-test": null,
  "val-examples-per-class": null,
  "batch-size": 4,
  "out": "/work3/fasco/data/processed/prompts/stt/ucf101",
  "num-workers": 2,
  "re-process-data": false,
  "use-wandb": true,
  "num-head-attention-pooling": 1,
  "N-text-layers": 12,
  "N-vision-layers": 12
}