{
  "clip-model": "openai/clip-vit-base-patch16",
  "ctx-len": 0,
  "ctx-len-video": 8,
  "class-wise": false,
  "lr": 8e-05,
  "weight-decay": 0.01,
  "std-init": 0.02,
  "num-temporal-views": 4,
  "num-spatial-views": 3,
  "num-frames-temporal": 8,
  "regularisation-strength": 0.0,
  "regularisation-text": "A video of a person doing",
  "cosine-regularisation-strength": 0.0,
  "use-handcrafted-features": true,
  "keep-vision-prompts-throughout": false,
  "temporal-pooling": "attention",
  "num-heads-attention-pooling": 1,
  "num-frames": null,
  "has-discriminative-conditioning": false,
  "space-between-frames": null,
  "epochs": 10,
  "start-train-step": 0,
  "epochs-to-skip": 0,
  "save-every-n-steps": 10000000000.0,
  "continue-from": null,
  "grad-norm-max": 1.0,
  "loss-function": "cross-entropy",
  "dataset-config": {
    "batch-size": 4,
    "num-workers": 4,
    "batches-per-backprop": 32,
    "split-file": "",
    "K-train": null,
    "K-val": null,
    "K-test": null,
    "split-function": "",
    "random-seed": 42,
    "has-validation": true,
    "use-cache": false,
    "resized-cache-protocol": null,
    "save-to-cache": false,
    "class-protocol": "all",
    "num-classes": null,
    "use-excess-for": "none",
    "remove-class-if-insufficient": false,
    "class-weights": null,
    "skip-first-K-train": 0,
    "skip-first-K-val": 0,
    "val-proportion": 0.2,
    "use-clips": false
  },
  "use-checkpointing": true,
  "debug": false,
  "use-profiling": false,
  "use-fresh-lr-scheduler": true,
  "learn-just-pooling": false,
  "checkpoint-last-n": 12,
  "log-every-n-steps": 10,
  "early-stop-patience": 3,
  "training-history": {
    "train-acc": [
      0.20516047828823158,
      0.5773442416614223,
      0.7228445563247325,
      0.7879169288860919,
      0.8152297042164883,
      0.8368785399622404,
      0.8548772813089994,
      0.8631843926998112,
      0.8649465072372562,
      0.8691000629326621
    ],
    "train-loss": [
      3.9149863318124605,
      2.6531173792479548,
      2.074340927863436,
      1.757307686634676,
      1.5676008643407804,
      1.4460748210352423,
      1.3697499164175582,
      1.3243473194619257,
      1.2987874400173065,
      1.2871495683212713
    ],
    "val-acc": [
      0.3413098236775819,
      0.44584382871536526,
      0.4981108312342569,
      0.5358942065491183,
      0.5680100755667506,
      0.5831234256926953,
      0.5969773299748111,
      0.5963476070528967,
      0.6045340050377834,
      0.6026448362720404
    ],
    "val-loss": [
      3.503341629312981,
      2.8917699857082413,
      2.5833477835811354,
      2.4047632787930273,
      2.2842923971807925,
      2.2122111278457064,
      2.163443220540018,
      2.143523636032412,
      2.1270060222455176,
      2.1237329189813408
    ]
  },
  "fp16": false,
  "grad-scaler-config": {
    "init-scale": 8192,
    "growth-factor": 2.0,
    "backoff-factor": 0.5,
    "growth-interval": 100
  },
  "activation-clamp-value": 10000.0,
  "method-name": "vidop",
  "prompt-optimisation-method": "vidop",
  "dataset": "ucf101",
  "data-dir": "/work3/fasco/data/raw/datasets/pevogam/ucf101/versions/1/UCF101/UCF-101",
  "model": "openai/clip-vit-base-patch16",
  "K": null,
  "K-val": null,
  "K-test": null,
  "val-examples-per-class": null,
  "batch-size": 4,
  "out": "/work3/fasco/data/processed/prompts/vidop/ucf101",
  "num-workers": 4,
  "re-process-data": false,
  "use-wandb": true,
  "num-head-attention-pooling": 1
}